{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SARSA-ExperienceReplay.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part I: On-policy learning and SARSA \n",
        "\n",
        "The policy we're gonna use is epsilon-greedy policy, where agent takes optimal action with probability , otherwise samples action at random. Note that agent can occasionally sample optimal action during random sampling by pure chance."
      ],
      "metadata": {
        "id": "H01UoLeXv2dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P9D6PVL6dQyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5853f8-cccc-41fb-becf-b507b6acd837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155629 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "zkWZgi6GwGla"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
        "        Instance variables you have access to\n",
        "          - self.epsilon (exploration prob)\n",
        "          - self.alpha (learning rate)\n",
        "          - self.discount (discount rate aka gamma)\n",
        "\n",
        "        Functions you should use\n",
        "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
        "            which returns legal actions for a state\n",
        "          - self.get_qvalue(state,action)\n",
        "            which returns Q(state,action)\n",
        "          - self.set_qvalue(state,action,value)\n",
        "            which sets Q(state,action) := value\n",
        "        !!!Important!!!\n",
        "        Note: please avoid using self._qValues directly. \n",
        "            There's a special self.get_qvalue/set_qvalue for that.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    #---------------------START OF YOUR CODE---------------------#\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        value = -np.inf\n",
        "        for a in possible_actions:\n",
        "            value = max(value, self.get_qvalue(state, a))\n",
        "\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        q_val = (1-learning_rate)*self.get_qvalue(state, action) + learning_rate*(reward + gamma * self.get_value(next_state))\n",
        "\n",
        "        self.set_qvalue(state, action, q_val)\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "        val = -np.inf\n",
        "        best_action = None\n",
        "        for a in possible_actions:\n",
        "            if self.get_qvalue(state, a) > val:\n",
        "                val = self.get_qvalue(state, a)\n",
        "                best_action = a\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action (self.get_best_action).\n",
        "\n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        if np.random.uniform(0,1) < epsilon:\n",
        "            action = np.random.choice(possible_actions)\n",
        "        else:\n",
        "            action = self.get_best_action(state)\n",
        "\n",
        "        return action"
      ],
      "metadata": {
        "id": "6sk5nmx2wIgu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we gonna implement Expected Value SARSA on top of it."
      ],
      "metadata": {
        "id": "oWOAW7suwPCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EVSarsaAgent(QLearningAgent):\n",
        "    \"\"\" \n",
        "    An agent that changes some of q-learning functions to implement Expected Value SARSA. \n",
        "    Note: this demo assumes that your implementation of QLearningAgent.update uses get_value(next_state).\n",
        "    If it doesn't, please add\n",
        "        def update(self, state, action, reward, next_state):\n",
        "            and implement it for Expected Value SARSA's V(s')\n",
        "    \"\"\"\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\" \n",
        "        Returns Vpi for current state under epsilon-greedy policy:\n",
        "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
        "\n",
        "        Hint: all other methods from QLearningAgent are still accessible.\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        state_value = 0\n",
        "\n",
        "        for action in possible_actions:\n",
        "            state_value += self.get_qvalue(state, action)\n",
        "        return state_value/(len(possible_actions))\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        q_val = (1-learning_rate)*self.get_qvalue(state, action) + learning_rate*(reward + gamma * self.get_value(next_state))\n",
        "\n",
        "        self.set_qvalue(state, action, q_val)"
      ],
      "metadata": {
        "id": "cUBo5jlJwMne"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cliff World\n",
        "Let's now see how our algorithm compares against q-learning in case where we force agent to explore all the time"
      ],
      "metadata": {
        "id": "-JRWFRzQwbw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym.envs.toy_text\n",
        "env = gym.envs.toy_text.CliffWalkingEnv()\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(env.__doc__)"
      ],
      "metadata": {
        "id": "gsL-1symwd_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b494a8-0955-4b48-b4fd-e37fbb5f587c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    This is a simple implementation of the Gridworld Cliff\n",
            "    reinforcement learning task.\n",
            "\n",
            "    Adapted from Example 6.6 (page 106) from Reinforcement Learning: An Introduction\n",
            "    by Sutton and Barto:\n",
            "    http://incompleteideas.net/book/bookdraft2018jan1.pdf\n",
            "\n",
            "    With inspiration from:\n",
            "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
            "\n",
            "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
            "        [3, 0] as the start at bottom-left\n",
            "        [3, 11] as the goal at bottom-right\n",
            "        [3, 1..10] as the cliff at bottom-center\n",
            "\n",
            "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward\n",
            "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our cliffworld has one difference from what's on the image: there is no wall.\n",
        "# Agent can choose to go as close to the cliff as it wishes. x:start, T:exit, C:cliff, o: flat ground\n",
        "env.render()"
      ],
      "metadata": {
        "id": "-xpK00CGwgx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9d1774-8021-4fa9-ba42-a650b9097be8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"This function should \n",
        "    - run a full game, actions given by agent.get_action(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        agent.update(s, a, r, next_s)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "-6VJIKkywig-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_sarsa = EVSarsaAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                           get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_ql = QLearningAgent(alpha=0.25, epsilon=0.2, discount=0.99,\n",
        "                          get_legal_actions=lambda s: range(n_actions))"
      ],
      "metadata": {
        "id": "rcP4rxTBwkpD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "rewards_sarsa, rewards_ql = [], []\n",
        "\n",
        "for i in range(5000):\n",
        "    rewards_sarsa.append(play_and_train(env, agent_sarsa))\n",
        "    rewards_ql.append(play_and_train(env, agent_ql))\n",
        "    # Note: agent.epsilon stays constant\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('EVSARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
        "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
        "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
        "        plt.plot(moving_average(rewards_sarsa), label='ev_sarsa')\n",
        "        plt.plot(moving_average(rewards_ql), label='qlearning')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.ylim(-500, 0)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9ysT9tjjwmUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "be43e565-d614-456e-dbb1-453e3abe7605"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVSARSA mean reward = -29.08\n",
            "QLEARNING mean reward = -63.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see what did the algorithms learn by visualizing their actions at every state."
      ],
      "metadata": {
        "id": "zSQYoacnwrKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_policy(env, agent):\n",
        "    \"\"\" Prints CliffWalkingEnv policy with arrows. Hard-coded. \"\"\"\n",
        "    n_rows, n_cols = env._cliff.shape\n",
        "\n",
        "    actions = '^>v<'\n",
        "\n",
        "    for yi in range(n_rows):\n",
        "        for xi in range(n_cols):\n",
        "            if env._cliff[yi, xi]:\n",
        "                print(\" C \", end='')\n",
        "            elif (yi * n_cols + xi) == env.start_state_index:\n",
        "                print(\" X \", end='')\n",
        "            elif (yi * n_cols + xi) == n_rows * n_cols - 1:\n",
        "                print(\" T \", end='')\n",
        "            else:\n",
        "                print(\" %s \" %\n",
        "                      actions[agent.get_best_action(yi * n_cols + xi)], end='')\n",
        "        print()"
      ],
      "metadata": {
        "id": "lZv8NM6cwo1j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-Learning\")\n",
        "draw_policy(env, agent_ql)\n",
        "\n",
        "print(\"SARSA\")\n",
        "draw_policy(env, agent_sarsa)"
      ],
      "metadata": {
        "id": "F5HBSY86ws3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0587d955-4140-4592-db3f-42337c14c17a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Learning\n",
            " >  >  v  >  >  v  >  v  >  >  v  v \n",
            " >  >  >  >  >  >  >  >  >  >  >  v \n",
            " >  >  >  >  >  >  >  >  >  >  >  v \n",
            " X  C  C  C  C  C  C  C  C  C  C  T \n",
            "SARSA\n",
            " >  >  >  >  >  >  >  >  >  >  >  v \n",
            " ^  >  ^  >  >  >  >  >  ^  ^  >  v \n",
            " ^  ^  ^  ^  ^  ^  ^  ^  ^  <  >  v \n",
            " X  C  C  C  C  C  C  C  C  C  C  T \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: experience replay (4 points)\n",
        "There's a powerful technique that you can use to improve sample efficiency for off-policy algorithms: [spoiler] Experience replay :)\n",
        "\n",
        "The catch is that you can train Q-learning and EV-SARSA on `<s,a,r,s'>` tuples even if they aren't sampled under current agent's policy. So here's what we're gonna do:\n",
        "\n",
        "Training with experience replay:\n",
        "* Play game, sample <s,a,r,s'>.\n",
        "* Update q-values based on <s,a,r,s'>.\n",
        "* Store <s,a,r,s'> transition in a buffer.\n",
        "  * If buffer is full, delete earliest data.\n",
        "* Sample K such transitions from that buffer and update q-values based on them"
      ],
      "metadata": {
        "id": "dkds1SWkD7hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "metadata": {
        "id": "sHZOBXvmEST1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "P2vT0jUjEVTS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Create Replay buffer.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "\n",
        "        Note: for this assignment you can pick any data structure you want.\n",
        "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
        "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "\n",
        "        # OPTIONAL: YOUR CODE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        '''\n",
        "        Make sure, _storage will not exceed _maxsize. \n",
        "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
        "        '''\n",
        "        data = (obs_t, action, reward, obs_tp1, done)  \n",
        "        # add data to storage\n",
        "        if self.__len__() < self._maxsize:\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage.pop(0)\n",
        "            self._storage.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch of experiences.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            batch of observations\n",
        "        act_batch: np.array\n",
        "            batch of actions executed given obs_batch\n",
        "        rew_batch: np.array\n",
        "            rewards received as results of executing act_batch\n",
        "        next_obs_batch: np.array\n",
        "            next set of observations seen after executing act_batch\n",
        "        done_mask: np.array\n",
        "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
        "            the end of an episode and 0 otherwise.\n",
        "        \"\"\"\n",
        "\n",
        "        # collect <s,a,r,s',done> for each index\n",
        "\n",
        "        idxes = []\n",
        "        for _ in range(self._maxsize):\n",
        "            idxes.append(random.randint(0, self._maxsize - 1))\n",
        "\n",
        "        obs_batch = []\n",
        "        act_batch = []\n",
        "        rew_batch = []\n",
        "        next_obs_batch = []\n",
        "        done_mask = []\n",
        "        if self.__len__() == 1:\n",
        "            obs_batch.append(self._storage[0][0])\n",
        "            act_batch.append(self._storage[0][1])\n",
        "            rew_batch.append(self._storage[0][2])\n",
        "            next_obs_batch.append(self._storage[0][3])\n",
        "            done_mask.append(self._storage[0][4])\n",
        "\n",
        "        else:  \n",
        "          for idx in idxes:\n",
        "              obs_batch.append(self._storage[idx][0])\n",
        "              act_batch.append(self._storage[idx][1])\n",
        "              rew_batch.append(self._storage[idx][2])\n",
        "              next_obs_batch.append(self._storage[idx][3])\n",
        "              done_mask.append(self._storage[idx][4])\n",
        "        \n",
        "        return (\n",
        "            obs_batch,\n",
        "            act_batch,\n",
        "            rew_batch,\n",
        "            next_obs_batch,\n",
        "            done_mask,\n",
        "        )"
      ],
      "metadata": {
        "id": "PiPRfhbSEXCm"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some tests to make sure your buffer works right\n"
      ],
      "metadata": {
        "id": "-8DH-U3IEcG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obj2arrays(obj):\n",
        "    for x in obj:\n",
        "        yield np.array([x])\n",
        "\n",
        "def obj2sampled(obj):\n",
        "    return tuple(obj2arrays(obj))\n",
        "\n",
        "replay = ReplayBuffer(2)\n",
        "obj1 = (0, 1, 2, 3, True)\n",
        "obj2 = (4, 5, 6, 7, False)\n",
        "replay.add(*obj1)\n",
        "assert replay.sample(1) == obj2sampled(obj1), \\\n",
        "    \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
        "replay.add(*obj2)\n",
        "assert len(replay) == 2, \"Please make sure __len__ methods works as intended.\"\n",
        "replay.add(*obj2)\n",
        "assert len(replay) == 2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
        "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj2)\n",
        "print(max(len(np.unique(a)) for a in replay.sample(100)))\n",
        "assert max(len(np.unique(a)) for a in replay.sample(100)) == 2\n",
        "replay.add(*obj1)\n",
        "assert tuple(np.unique(a) for a in replay.sample(100)) == obj2sampled(obj1)\n",
        "print(\"Success!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRY7cCLiEct8",
        "outputId": "d36b00d0-3ade-4465-854d-ab62a903c7dd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "63OjJKP4Ee1O"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_and_train_with_replay(env, agent, replay=None,\n",
        "                               t_max=10**4, replay_batch_size=32):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent.get_action(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\n",
        "    :param replay: ReplayBuffer where agent can store and sample (s,a,r,s',done) tuples.\n",
        "        If None, do not use experience replay\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s\n",
        "        a = <YOUR CODE>\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "\n",
        "        # update agent on current transition. Use agent.update\n",
        "        <YOUR CODE>\n",
        "\n",
        "        if replay is not None:\n",
        "            # store current <s,a,r,s'> transition in buffer\n",
        "            <YOUR CODE>\n",
        "\n",
        "            # sample replay_batch_size random transitions from replay,\n",
        "            # then update agent on each of them in a loop\n",
        "            s_, a_, r_, next_s_, done_ = replay.sample(replay_batch_size)\n",
        "            for i in range(replay_batch_size):\n",
        "                <YOUR CODE>\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "WizkEDVTEgdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two agents: first will use experience replay, second will not.\n",
        "\n",
        "agent_baseline = QLearningAgent(\n",
        "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "    get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "agent_replay = QLearningAgent(\n",
        "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "    get_legal_actions=lambda s: range(n_actions))\n",
        "\n",
        "replay = ReplayBuffer(1000)"
      ],
      "metadata": {
        "id": "8lBkCC3TEjoI"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "\n",
        "def moving_average(x, span=100):\n",
        "    return pd.DataFrame({'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "rewards_replay, rewards_baseline = [], []\n",
        "\n",
        "for i in range(1000):\n",
        "    rewards_replay.append(\n",
        "        play_and_train_with_replay(env, agent_replay, replay))\n",
        "    rewards_baseline.append(\n",
        "        play_and_train_with_replay(env, agent_baseline, replay=None))\n",
        "\n",
        "    agent_replay.epsilon *= 0.99\n",
        "    agent_baseline.epsilon *= 0.99\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print('Baseline : eps =', agent_replay.epsilon,\n",
        "              'mean reward =', np.mean(rewards_baseline[-10:]))\n",
        "        print('ExpReplay: eps =', agent_baseline.epsilon,\n",
        "              'mean reward =', np.mean(rewards_replay[-10:]))\n",
        "        plt.plot(moving_average(rewards_replay), label='exp. replay')\n",
        "        plt.plot(moving_average(rewards_baseline), label='baseline')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "kTnGmV0SEle1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}